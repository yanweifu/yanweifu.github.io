<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <meta charset="utf-8">
        <title>Transductive Multi-view Embedding for Zero-Shot Recognition and Annotation
</title>
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="description" content="Yanwei Fu">
        <meta name="author" content="Yanwei Fu">
        
        <meta name="keywords" content="Yanwei Fu, USAA, Multi-label zero-shot learning, natural-scene, IPARPC-12, QMUL, computer vision, image annotation, zero-shot description" />
        
        <!-- Le styles -->
        <link href="./css/bootstrap.css" rel="stylesheet">
        <link href="./css/bootstrap-responsive.css" rel="stylesheet">
        <link href="./css/docs.css" rel="stylesheet">
        <link href="./prettify.css" rel="stylesheet">
        <link href="./css/cavan.css" rel="stylesheet">
        
        
        <!-- Le HTML5 shim, for IE6-8 support of HTML5 elements -->
        <!--[if lt IE 9]>
      <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

        <!-- Le fav and touch icons -->
        <link rel="shortcut icon" type="image/ico" href="http://www.eecs.qmul.ac.uk/~ccloy/favicon.ico" /> 
        <link rel="apple-touch-icon-precomposed" sizes="114x114" href="http://twitter.github.com/bootstrap/assets/ico/apple-touch-icon-114-precomposed.png">
        <link rel="apple-touch-icon-precomposed" sizes="72x72" href="http://twitter.github.com/bootstrap/assets/ico/apple-touch-icon-72-precomposed.png">
        <link rel="apple-touch-icon-precomposed" href="http://twitter.github.com/bootstrap/assets/ico/apple-touch-icon-57-precomposed.png">
    </head>
    
    <body>
        
        <div id="navbar" class="navbar navbar-inverse navbar-fixed-top">
            <div class="navbar-inner">
     
            </div>
        </div>
        
        <div class="container">
            <div class="tooltip-demo">
                
                
                <!--<ul class="breadcrumb">
                <li><a href="index.html#home">Home</a> <span class="divider">/</span></li>
                <li><a href="index.html#thumbnails">Download</a> <span class="divider">/</span></li>
                <li class="active">QMUL Underground Multi-camera Dataset</li>
            </ul>-->
            
            
                <!-- Home ================================================== -->
                <section>
				<div class="page-header">
					<h2 style="color: rgb(0, 0, 0); font-family: Simsun; font-style: normal; font-variant: normal; letter-spacing: normal; line-height: normal; orphans: auto; text-align: center; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;">
					Transductive Multi-label Zero-shot Learning  </h2>
					<p style="color: rgb(0, 0, 0); font-family: Simsun; font-style: normal; font-variant: normal; letter-spacing: normal; line-height: normal; orphans: auto; text-align: center; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;">
					<span style="color: rgb(0, 0, 0); font-family: Simsun; font-size: medium; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: auto; text-align: center; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px; display: inline !important; float: none;">
					Yanwei Fu, Yongxing Yang, Timothy M. Hospedales, Tao Xiang and Shaogang Gong</span><br style="color: rgb(0, 0, 0); font-family: Simsun; font-size: medium; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: auto; text-align: center; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;">
					<span style="color: rgb(0, 0, 0); font-family: Simsun; font-size: medium; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: auto; text-align: center; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px; display: inline !important; float: none;">
					School of EECS, Queen Mary University of London, UK 
					{y.fu,yongxin.yang,t.hospedales, t.xiang, s.gong}@qmul.ac.uk</span></p>
					</div>

				<div align="left">
					<p>  <h3> Abstract </h3>
						 Zero-shot learning has received increasing interest as a means to alleviate the often prohibitive expense of annotating training data for large scale recognition problems.
These methods have achieved great success via learning intermediate semantic representations in the form of attributes and more recently, semantic word vectors. However,
they have thus far been constrained to the single-label case, in contrast to the growing popularity and importance of more realistic multi-label data. In this paper, for the first
time, we investigate and formalise a general framework for multi-label zero-shot learning, addressing the unique challenge therein: how to exploit multi-label correlation at test
time with no training data for those classes? In particular, we propose (1) a multi-output deep regression model to project an image into a semantic word space, which explicitly
exploits the correlations in the intermediate semantic layer of word vectors; (2) a novel zero-shot learning algorithm for multi-label data that exploits the unique composition-
ality property of semantic word vector representations; and (3) a transductive learning strategy to enable the regression model learned from seen classes to generalise well to
unseen classes. Our zero-shot learning experiments on a number of standard multi-label datasets demonstrate that our method outperforms a variety of baselines.
					</p>
				</div>
<br />
<a href ='transductive_multilabel_abstract.pdf'> 1-page abstract</a>

<br />

<img src="./multilabelZSL.jpg" alt="pipeline of our algorithm" align="middle" height="900" width="95%"  > </a> 




					<div class="span3">
						<h3>Download</h3>   
						<p>
						<a href="./mat/readme.txt" class="btn btn-success">
						<i class="icon-download-alt icon-white"></i>Download-readme</a>
						<br /><em>data for several dataset used in our papers</em>
						</p>

						 <p> We are using all wikipedia articles to train the google word2vec recurrent neural network model <br />
						 and generate the semantic word vector dictionary; and we will release it to the community.</p> 	
						<h3> Paper: </h3>
[1] Fu, Yanwei; Yang, Yongxin, Hospedales, T.; Xiang, T.;  Gong, S: Transductive Multi-label Zero-Shot learning, (BMVC 2014). <a href='transductive_ZSL_on_multilabel_yanwei.pdf'> Paper</a>
<p>bib: @INPROCEEDINGS{embedding2014ECCV, <br/>
  author = { Yanwei Fu and Yongxing Yang and Timothy M. Hospedales and Tao Xiang and Shaogang Gong}, <br/>
  title = {Transductive Multi-label Zero-Shot Learning}, <br/>
  booktitle ={BMVC}, <br/>
  year = {2014}<br/>
}


[2] Fu, Yanwei; Hospedales, T.; Xiang, T.; Fu, Z.; Gong, S: Transductive Multi-view Embedding for Zero-Shot Recognition and Annotation, (ECCV 2014). <a href ="embedding_paper_eccv14.pdf"> Paper</a>
<p>bib: @INPROCEEDINGS{embedding2014ECCV, <br/>
  author = { Yanwei Fu and Timothy M. Hospedales and Tao Xiang and Zhenyong Fu and Shaogang Gong}, <br/>
  title = {Transductive Multi-view Embedding for Zero-Shot Recognition and Annotation}, <br/>
  booktitle = {ECCV}, <br/>
  year = {2014}<br/>
}
</p>

[3] Fu, Yanwei, Hospedales, T.M., Xiang, T., Gong, S.: Transductive multi-view zero-shot recognition and annotation. Submitted to IEEE TPAMI (2014)
<p>bib: @INPROCEEDINGS{embedding2014ECCV, <br/>
  author = { Yanwei Fu and Timothy M. Hospedales and Tao Xiang and Shaogang Gong}, <br/>
  title = {Transductive multi-view zero-shot recognition and annotation}, <br/>
  booktitle = {submitted to TPAMI}, <br/>
  year = {2014}<br/>
}
</p>

				        </div>
				</div>

				


        
</body></html>
